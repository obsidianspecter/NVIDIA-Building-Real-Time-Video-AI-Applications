{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac135fc",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc304c1c",
   "metadata": {},
   "source": [
    "# Building a Multi-DNN DeepStream Application #\n",
    "DeepStream pipelines can be constructed to perform complex analytics that involve multiple neural networks. One common use case for this would be to use a detector as a primary inference engine to localize an object and a classifier as a secondary inference engine. This is useful since classification models can often perform better on single objects within the frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc4b846",
   "metadata": {},
   "source": [
    "## Learning Objectives ##\n",
    "In this notebook, you will learn how to build a Multi-DNN DeepStream pipeline using Python, including: \n",
    "* Planning the Pipeline Architecture\n",
    "* Using Specification File to Configure Deep Learning Inference\n",
    "* Handling Metadata\n",
    "\n",
    "**Table of Contents**\n",
    "<br>\n",
    "This notebook covers the below sections:  \n",
    "1. [Designing the Pipeline](#s1)\n",
    "    * [Exercise #1 - Preview the Input Video](#e1)\n",
    "2. [Preparing the Deep Learning Models](#s2)\n",
    "    * [Exercise #2 - Download TrafficCamNet and VehicleTypeNet Models](#e2)\n",
    "3. [Building a Video AI Application](#s3)\n",
    "    * [Pipeline Components](#s3.1)\n",
    "    * [Exercise #3 - Initializing GStreamer and Pipeline](#e3)\n",
    "    * [Exercise #4 - Creating Pipeline Elements](#e4)\n",
    "    * [Exercise #5 - Modify the GIE Configuration Files](#e5)\n",
    "    * [Exercise #6 - Linking Pipeline Elements](#e6)\n",
    "    * [Exercise #7 - Add Probe to OSD Sink](#e7)\n",
    "    * [Exercise #8 - Starting the Pipeline](#e8)\n",
    "    * [Viewing Inference Results](#s3.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f53f38",
   "metadata": {},
   "source": [
    "<a name='s1'></a>\n",
    "## Designing the Pipeline ##\n",
    "Building a video AI application begins by designing the project based on the use case. For this activity, we will build a DeepStream pipeline that will accurately detect cars and classify the vehicle type from a parking garage camera feed. We will use pre-trained models available from NGC. Let's begin by looking at the raw input video and use the `ffprobe` command line utility to understand its format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a06a8a4",
   "metadata": {},
   "source": [
    "<a name='e1'></a>\n",
    "#### Exercise #1 - Preview the Input Video ####\n",
    "\n",
    "**Instructions**: <br>\n",
    "* Execute the cell to set the environment variables. \n",
    "* Execute the cell below to preview the input .mp4 video. \n",
    "* Modify the `<FIXME>`s only and execute the cell below to study the input video. \n",
    "* Mark the video properties in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df6076dc-a478-4dcc-b6e8-c1f917e0940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "import os\n",
    "\n",
    "# Set the input video path to an environment variable\n",
    "os.environ['TARGET_VIDEO_PATH']='data/sample_30.h264'\n",
    "os.environ['TARGET_VIDEO_PATH_MP4']='sample_30.mp4'\n",
    "\n",
    "target_video_path=os.environ['TARGET_VIDEO_PATH']\n",
    "target_video_path_mp4=os.environ['TARGET_VIDEO_PATH_MP4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56154469",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"sample_30.mp4\" controls  width=\"720\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "from IPython.display import Video\n",
    "\n",
    "# Convert the H.264 encoded video file to MP4 container file - this will generate the sample_30.mp4 file\n",
    "!ffmpeg -i $TARGET_VIDEO_PATH $TARGET_VIDEO_PATH_MP4 \\\n",
    "        -y \\\n",
    "        -loglevel quiet\n",
    "\n",
    "# View the input video\n",
    "Video(target_video_path_mp4, width=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40f7c58d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffprobe version 3.4.8-0ubuntu0.2 Copyright (c) 2007-2020 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "Input #0, h264, from 'data/sample_30.h264':\n",
      "  Duration: N/A, bitrate: N/A\n",
      "    Stream #0:0: Video: h264 (High), yuv420p(progressive), 882x692, 30 fps, 30 tbr, 1200k tbn, 60 tbc\n"
     ]
    }
   ],
   "source": [
    "!ffprobe -i $TARGET_VIDEO_PATH"
   ]
  },
  {
   "cell_type": "raw",
   "id": "007421cb-7d10-4cec-a601-6b87ae3d0dd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Video Codec: h264\n",
    "Video Color Format: yuv\n",
    "Width: 882\n",
    "Height: 692\n",
    "Frame Rate: 30 fps"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17c01ce2-88c1-4bf9-80bf-e7339d0bcfde",
   "metadata": {
    "tags": []
   },
   "source": [
    "!ffprobe -i $TARGET_VIDEO_PATH\n",
    "\n",
    "# Video Codec: h264\n",
    "# Video Color Format: yuv\n",
    "# Width: 882\n",
    "# Height: 692\n",
    "# Frame Rate: 30 fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea7d54a",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05673df1",
   "metadata": {},
   "source": [
    "<a name='s2'></a>\n",
    "## Preparing the Deep Learning Models ##\n",
    "We'll be using two purpose-built models from NGC - the [TrafficCamNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/trafficcamnet) object detection model and the [VehicleTypeNet](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/vehicletypenet) classification model. We need to download and install the NGC CLI before using it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1008d59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLI=ngccli_cat_linux.zip\n",
      "--2025-02-14 10:13:22--  https://ngc.nvidia.com/downloads/ngccli_cat_linux.zip\n",
      "Resolving ngc.nvidia.com (ngc.nvidia.com)... 18.165.83.53, 18.165.83.59, 18.165.83.119, ...\n",
      "Connecting to ngc.nvidia.com (ngc.nvidia.com)|18.165.83.53|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 48777813 (47M) [application/zip]\n",
      "Saving to: ‘/dli/task/ngc_assets/ngccli/ngccli_cat_linux.zip’\n",
      "\n",
      "ngccli_cat_linux.zi 100%[===================>]  46.52M   265MB/s    in 0.2s    \n",
      "\n",
      "2025-02-14 10:13:23 (265 MB/s) - ‘/dli/task/ngc_assets/ngccli/ngccli_cat_linux.zip’ saved [48777813/48777813]\n",
      "\n",
      "Archive:  /dli/task/ngc_assets/ngccli/ngccli_cat_linux.zip\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "import os\n",
    "os.environ['NGC_DIR']='/dli/task/ngc_assets'\n",
    "\n",
    "# Download and install NGC CLI - this will create the ngc_assets folder\n",
    "%env CLI=ngccli_cat_linux.zip\n",
    "!mkdir -p $NGC_DIR/ngccli\n",
    "!wget \"https://ngc.nvidia.com/downloads/$CLI\" -P $NGC_DIR/ngccli\n",
    "!unzip -o \\\n",
    "       -u \"$NGC_DIR/ngccli/$CLI\" \\\n",
    "       -d $NGC_DIR/ngccli/\n",
    "!rm $NGC_DIR/ngccli/*.zip\n",
    "os.environ[\"PATH\"]=\"{}/ngccli/ngc-cli:{}\".format(os.getenv(\"NGC_DIR\", \"\"), os.getenv(\"PATH\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df659137-7294-4cf0-841a-850ba50b5126",
   "metadata": {},
   "source": [
    "<a name='e2'></a>\n",
    "#### Exercise #2 - Download TrafficCamNet and VehicleTypeNet Models ####\n",
    "\n",
    "**Instructions**: <br>\n",
    "* Modify the `<FIXME>`s only and execute the cell to download the NGC models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd5bd643",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting files to download...\n",
      "\u001b[2K\u001b[32m⠙\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K  \u001b[90m━━\u001b[0m • \u001b[32m…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0…\u001b[0m • \u001b[31m…\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 3 - Failed: 0\u001b[0m\n",
      "       \u001b[32m…\u001b[0m                   \u001b[31m…\u001b[0m                                                    \n",
      "\u001b[?25h\n",
      "-------------------------------------------------------------------------------\n",
      "   Download status: COMPLETED\n",
      "   Downloaded local path model: /dli/task/ngc_assets/trafficcamnet_vpruned_v1.0\n",
      "   Total files downloaded: 3\n",
      "   Total transferred: 5.2 MB\n",
      "   Started at: 2025-02-14 10:13:32\n",
      "   Completed at: 2025-02-14 10:13:33\n",
      "   Duration taken: 0s\n",
      "-------------------------------------------------------------------------------\n",
      "Getting files to download...\n",
      "\u001b[2K\u001b[32m⠙\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠴\u001b[0m \u001b[36m━━\u001b[0m • \u001b[32m0…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m…\u001b[0m • \u001b[31m?\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 0 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠦\u001b[0m \u001b[33m━\u001b[0m\u001b[36m╺\u001b[0m • \u001b[32m…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0…\u001b[0m • \u001b[31m…\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 2 - Failed: 0\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K  \u001b[90m━━\u001b[0m • \u001b[32m…\u001b[0m • \u001b[36mRemaining:\u001b[0m \u001b[36m0…\u001b[0m • \u001b[31m…\u001b[0m • \u001b[33mElapsed:\u001b[0m \u001b[33m0…\u001b[0m • \u001b[34mTotal: 3 - Completed: 3 - Failed: 0\u001b[0m\n",
      "       \u001b[32m…\u001b[0m                   \u001b[31m…\u001b[0m                                                    \n",
      "\u001b[?25h\n",
      "--------------------------------------------------------------------------------\n",
      "   Download status: COMPLETED\n",
      "   Downloaded local path model: /dli/task/ngc_assets/vehicletypenet_vpruned_v1.0\n",
      "   Total files downloaded: 3\n",
      "   Total transferred: 19.06 MB\n",
      "   Started at: 2025-02-14 10:13:36\n",
      "   Completed at: 2025-02-14 10:13:36\n",
      "   Duration taken: 0s\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Download the purpose-built TrafficCamNet model from NGC\n",
    "!ngc registry model download-version nvidia/tao/trafficcamnet:pruned_v1.0 --dest $NGC_DIR\n",
    "\n",
    "# Download the purpose-built VehicleTypeNet model from NGC\n",
    "!ngc registry model download-version nvidia/tao/vehicletypenet:pruned_v1.0 --dest $NGC_DIR"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33c8748b-4a59-4dc3-8853-0ce3fb21bb88",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Download the purpose-built TrafficCamNet model from NGC\n",
    "!ngc registry model download-version nvidia/tao/trafficcamnet:pruned_v1.0 --dest $NGC_DIR\n",
    "\n",
    "# Download the purpose-built VehicleTypeNet model from NGC\n",
    "!ngc registry model download-version nvidia/tao/vehicletypenet:pruned_v1.0 --dest $NGC_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23d19aa-fe07-4968-925b-574eede62321",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b716d37",
   "metadata": {},
   "source": [
    "<a name='s3'></a>\n",
    "## Building a Video AI Application ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210bf6a",
   "metadata": {},
   "source": [
    "<a name='s3.1'></a>\n",
    "### Pipeline Components ###\n",
    "This is the pipeline architecture of the application. We'll be using an object detection network to identify and localize the cars in the frames, followed by a secondary inference to classify vehicle types. \n",
    "<p><img src=\"images/deepstream_multi_gie_pipeline.png\" width='720'></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e0894d",
   "metadata": {},
   "source": [
    "<a name='e3'></a>\n",
    "#### Exercise #3 - Initializing GStreamer and Pipeline ####\n",
    "\n",
    "**Instructions**: <br>\n",
    "* Execute the below cell to import the necessary libraries. \n",
    "* Modify the `<FIXME>`s only and execute the cell below to initialize GStreamer and instantiate a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "166ffdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Import necessary GStreamer libraries and DeepStream python bindings\n",
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import GObject, Gst, GLib\n",
    "from common.bus_call import bus_call\n",
    "import pyds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f092b858",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created pipeline\n"
     ]
    }
   ],
   "source": [
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import Gst\n",
    "\n",
    "# Initialize GStreamer\n",
    "Gst.init(None)\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline=Gst.Pipeline()\n",
    "print('Created pipeline')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1d01c84-c603-4b0d-b356-3491b2809afb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initialize GStreamer\n",
    "Gst.init(None)\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline=Gst.Pipeline()\n",
    "print('Created pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a60ab-85b3-4baa-aa5a-ae903f6f6415",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495cb40-c038-4085-a034-22eeafc42674",
   "metadata": {},
   "source": [
    "<a name='e4'></a>\n",
    "#### Exercise #4 - Creating Elements ####\n",
    "\n",
    "**Instructions**: <br>\n",
    "* Modify the `<FIXME>` only and execute the below cell to creating the necessary pipeline elements and set their properties. \n",
    "* Execute the cell below to add the elements to the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bb384c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created elements\n"
     ]
    }
   ],
   "source": [
    "# Create Source element for reading from a file and set the location property\n",
    "source=Gst.ElementFactory.make(\"filesrc\", \"file-source\")\n",
    "source.set_property('location', \"data/sample_30.h264\")\n",
    "\n",
    "# Create H264 Parser with h264parse as the input file is an elementary h264 stream\n",
    "h264parser=Gst.ElementFactory.make(\"h264parse\", \"h264-parser\")\n",
    "\n",
    "# Create Decoder with nvv4l2decoder for accelerating decoding on GPU\n",
    "decoder=Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder\")\n",
    "\n",
    "# Create Streamux with nvstreammux to form batches for one or more sources and set properties\n",
    "streammux=Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\")\n",
    "streammux.set_property('width', 888) \n",
    "streammux.set_property('height', 696) \n",
    "streammux.set_property('batch-size', 1)\n",
    "\n",
    "# Create Primary GStreamer Inference Element with nvinfer to run inference on the decoder's output after batching\n",
    "pgie=Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\")\n",
    "\n",
    "# Create Secondary Inference Element with nvinfer to run inference on the pgie's output\n",
    "sgie=Gst.ElementFactory.make(\"nvinfer\", \"secondary-inference\")\n",
    "\n",
    "# Create Convertor to convert from YUV to RGBA as required by nvdsosd\n",
    "nvvidconv1=Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor1\")\n",
    "\n",
    "# Create OSD with nvdsosd to draw on the converted RGBA buffer\n",
    "nvosd=Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\")\n",
    "\n",
    "# Create Convertor to convert from RGBA to I420 as required by encoder\n",
    "nvvidconv2=Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor2\")\n",
    "\n",
    "# Create Capsfilter to enforce frame image format\n",
    "capsfilter=Gst.ElementFactory.make(\"capsfilter\", \"capsfilter\")\n",
    "caps=Gst.Caps.from_string(\"video/x-raw, format=I420\")\n",
    "capsfilter.set_property(\"caps\", caps)\n",
    "\n",
    "# Create Encoder to encode I420 formatted frames using the MPEG4 codec\n",
    "encoder=Gst.ElementFactory.make(\"avenc_mpeg4\", \"encoder\")\n",
    "encoder.set_property(\"bitrate\", 2000000)\n",
    "\n",
    "# Create Sink with fakesink as the end point of the pipeline\n",
    "sink=Gst.ElementFactory.make('filesink', 'filesink')\n",
    "sink.set_property('location', 'output_04_raw.mpeg4')\n",
    "sink.set_property(\"sync\", 1)\n",
    "print('Created elements')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d57360d7-8401-4122-80d8-6517b7e573f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create Source element for reading from a file and set the location property\n",
    "source=Gst.ElementFactory.make(\"filesrc\", \"file-source\")\n",
    "source.set_property('location', \"data/sample_30.h264\")\n",
    "\n",
    "# Create H264 Parser with h264parse as the input file is an elementary h264 stream\n",
    "h264parser=Gst.ElementFactory.make(\"h264parse\", \"h264-parser\")\n",
    "\n",
    "# Create Decoder with nvv4l2decoder for accelerating decoding on GPU\n",
    "decoder=Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder\")\n",
    "\n",
    "# Create Streamux with nvstreammux to form batches for one or more sources and set properties\n",
    "streammux=Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\")\n",
    "streammux.set_property('width', 888) \n",
    "streammux.set_property('height', 696) \n",
    "streammux.set_property('batch-size', 1)\n",
    "\n",
    "# Create Primary GStreamer Inference Element with nvinfer to run inference on the decoder's output after batching\n",
    "pgie=Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\")\n",
    "\n",
    "# Create Secondary Inference Element with nvinfer to run inference on the pgie's output\n",
    "sgie=Gst.ElementFactory.make(\"nvinfer\", \"secondary-inference\")\n",
    "\n",
    "# Create Convertor to convert from YUV to RGBA as required by nvdsosd\n",
    "nvvidconv1=Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor1\")\n",
    "\n",
    "# Create OSD with nvdsosd to draw on the converted RGBA buffer\n",
    "nvosd=Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\")\n",
    "\n",
    "# Create Convertor to convert from RGBA to I420 as required by encoder\n",
    "nvvidconv2=Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor2\")\n",
    "\n",
    "# Create Capsfilter to enforce frame image format\n",
    "capsfilter=Gst.ElementFactory.make(\"capsfilter\", \"capsfilter\")\n",
    "caps=Gst.Caps.from_string(\"video/x-raw, format=I420\")\n",
    "capsfilter.set_property(\"caps\", caps)\n",
    "\n",
    "# Create Encoder to encode I420 formatted frames using the MPEG4 codec\n",
    "encoder=Gst.ElementFactory.make(\"avenc_mpeg4\", \"encoder\")\n",
    "encoder.set_property(\"bitrate\", 2000000)\n",
    "\n",
    "# Create Sink with fakesink as the end point of the pipeline\n",
    "sink=Gst.ElementFactory.make('filesink', 'filesink')\n",
    "sink.set_property('location', 'output_04_raw.mpeg4')\n",
    "sink.set_property(\"sync\", 1)\n",
    "print('Created elements')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0de200-04b9-4757-a434-5a801d662b2f",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3a7a768-3b99-4260-b113-d5e292f1d61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added elements to pipeline\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Add elements to pipeline\n",
    "pipeline.add(source)\n",
    "pipeline.add(h264parser)\n",
    "pipeline.add(decoder)\n",
    "pipeline.add(streammux)\n",
    "pipeline.add(pgie)\n",
    "pipeline.add(sgie)\n",
    "pipeline.add(nvvidconv1)\n",
    "pipeline.add(nvosd)\n",
    "pipeline.add(nvvidconv2)\n",
    "pipeline.add(capsfilter)\n",
    "pipeline.add(encoder)\n",
    "pipeline.add(sink)\n",
    "print('Added elements to pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcf0e17-7774-4f4d-93d5-5cb0bc7eb173",
   "metadata": {},
   "source": [
    "<a name='e5'></a>\n",
    "#### Exercise #5 - Modify the GIE Configuration File(s) ####\n",
    "\n",
    "**Instructions**: <br>\n",
    "* Review the [primary gie configuration file](./spec_files/pgie_config_trafficcamnet_04.txt) (`./spec_files/pgie_config_trafficcamnet_04.txt`), which has been completed for you. \n",
    "* Modify the `<FIXME>`s only in the secondary gie configuration file, which has been started for you as [spec_files/sgie_config_vehicletypenet_04.txt](./spec_files/sgie_config_vehicletypenet_04.txt) (`./spec_files/sgie_config_vehicletypenet_04.txt`). \n",
    "* Execute the cell to set the `config-file-path` for the primary and secondary inference plugins. \n",
    "* Execute the cell below to modify the [labels.txt](./ngc_assets/vehicletypenet_vpruned_v1.0/labels.txt) to be appropriate for a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8afb090-feb6-4caa-b825-ffdd0bde1829",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[property]\n",
      "gpu-id=0\n",
      "net-scale-factor=1\n",
      "tlt-model-key=tlt_encode\n",
      "tlt-encoded-model=/dli/task/ngc_assets/vehicletypenet_vpruned_v1.0/resnet18_vehicletypenet_pruned.etlt\n",
      "labelfile-path=/dli/task/ngc_assets/vehicletypenet_vpruned_v1.0/labels.txt\n",
      "input-dims=3;224;224;0\n",
      "uff-input-blob-name=input_1\n",
      "batch-size=1\n",
      "# 0=FP32, 1=INT8, 2=FP16 mode\n",
      "network-mode=0\n",
      "network-type=1\n",
      "num-detected-classes=6\n",
      "model-color-format=0\n",
      "process-mode=2\n",
      "gie-unique-id=2\n",
      "operate-on-gie-id=1\n",
      "operate-on-class-ids=0\n",
      "output-blob-names=predictions/Softmax"
     ]
    }
   ],
   "source": [
    "!cat spec_files/sgie_config_vehicletypenet_04_soln.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f07b4c-93c6-4d6c-a40e-6b01a85e51a9",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53c38b41-ec4c-4c2d-a690-95e1e4b416ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Set the location of the config file\n",
    "pgie.set_property('config-file-path', 'spec_files/pgie_config_trafficcamnet_03.txt')\n",
    "# sgie.set_property('config-file-path', 'spec_files/sgie_config_vehicletypenet_04.txt')\n",
    "sgie.set_property('config-file-path', 'spec_files/sgie_config_vehicletypenet_04.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28d6ef0c-4fe2-41d4-b326-bc090aca7a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ngc_assets/vehicletypenet_vpruned_v1.0/labels.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ngc_assets/vehicletypenet_vpruned_v1.0/labels.txt\n",
    "\n",
    "coupe;largevehicle;sedan;suv;truck;van"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52834278-88f0-4b50-ad3e-cab1a8ff4267",
   "metadata": {},
   "source": [
    "<p><img src='images/tip.png' width=720></p>\n",
    "\n",
    "For classifiers, `labels.txt` should be semicolon delimited. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654ec82",
   "metadata": {},
   "source": [
    "<a name='e6'></a>\n",
    "#### Exercise #6 - Link Elements ####\n",
    "\n",
    "**Instructions**: <br>\n",
    "* Modify `<FIXME>`s only and execute the below cell to link elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d728a39d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked elements in pipeline\n"
     ]
    }
   ],
   "source": [
    "# Link elements together\n",
    "source.link(h264parser)\n",
    "h264parser.link(decoder)\n",
    "\n",
    "# Link decoder source pad to streammux sink pad\n",
    "decoder_srcpad=decoder.get_static_pad(\"src\")    \n",
    "streammux_sinkpad=streammux.get_request_pad(\"sink_0\")\n",
    "decoder_srcpad.link(streammux_sinkpad)\n",
    "\n",
    "# Link the rest of the elements in the pipeline\n",
    "streammux.link(pgie)\n",
    "pgie.link(sgie)\n",
    "sgie.link(nvvidconv1)\n",
    "nvvidconv1.link(nvosd)\n",
    "nvosd.link(nvvidconv2)\n",
    "nvvidconv2.link(capsfilter)\n",
    "capsfilter.link(encoder)\n",
    "encoder.link(sink)\n",
    "print('Linked elements in pipeline')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f06928d0-8cca-4426-a1e9-9ff88391d336",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Link elements together\n",
    "source.link(h264parser)\n",
    "h264parser.link(decoder)\n",
    "\n",
    "# Link decoder source pad to streammux sink pad\n",
    "decoder_srcpad=decoder.get_static_pad(\"src\")    \n",
    "streammux_sinkpad=streammux.get_request_pad(\"sink_0\")\n",
    "decoder_srcpad.link(streammux_sinkpad)\n",
    "\n",
    "# Link the rest of the elements in the pipeline\n",
    "streammux.link(pgie)\n",
    "pgie.link(sgie)\n",
    "sgie.link(nvvidconv1)\n",
    "nvvidconv1.link(nvosd)\n",
    "nvosd.link(nvvidconv2)\n",
    "nvvidconv2.link(capsfilter)\n",
    "capsfilter.link(encoder)\n",
    "encoder.link(sink)\n",
    "print('Linked elements in pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0860f9-3b5c-4235-854f-8d63fa7ab923",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba4d43-1c7a-48f4-81ca-a0906a4f4ab6",
   "metadata": {},
   "source": [
    "<a name='e7'></a>\n",
    "#### Exercise #7 - Add Probe to OSD Sink Pad ####\n",
    "\n",
    "**Instructions**: <br>\n",
    "* Execute the cell to define the `osd_sink_pad_buffer_probe` function. \n",
    "* Execute the cell below to define a helper `analyze_meta` function that analyzes the metadata generated by the secondary inference plugin. \n",
    "* Modify `<FIXME>`s only and execute the below cell to add the probe callback function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4398acd",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can use a similar probe function to access the metadata. However, in this case we also traverse the metadata generated from the secondary inference plugin. In this example our secondary inference was a classifier performed on the `car` class from the primary inference. We can access the metadata generated in `classifier_meta_list` after we cast it with `NvDsClassifierMeta.cast()`. Depending on how many secondary inferences there are, the `NvDsObjectMeta` object may have one or more `NvDsClassifierMeta` objects. We will also need to cast to `NvDsLabelInfo` class to get the resulting classification of the secondary inference(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e836a81-4adb-4dee-a9ab-ae7d855346ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Define the Probe Function\n",
    "def osd_sink_pad_buffer_probe(pad, info):\n",
    "    gst_buffer = info.get_buffer()\n",
    "\n",
    "    # Retrieve batch metadata from the gst_buffer\n",
    "    # Note that pyds.gst_buffer_get_nvds_batch_meta() expects the\n",
    "    # C address of gst_buffer as input, which is obtained with hash(gst_buffer)\n",
    "    batch_meta = pyds.gst_buffer_get_nvds_batch_meta(hash(gst_buffer))\n",
    "    l_frame = batch_meta.frame_meta_list\n",
    "\n",
    "    # Iterate through each frame in the batch metadata until the end\n",
    "    while l_frame is not None:\n",
    "        try:\n",
    "            frame_meta = pyds.NvDsFrameMeta.cast(l_frame.data)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        frame_num=frame_meta.frame_num\n",
    "        num_obj = frame_meta.num_obj_meta\n",
    "        l_obj=frame_meta.obj_meta_list\n",
    "        \n",
    "        print(\"Frame Number={} Number of Objects={}\".format(frame_num, num_obj))\n",
    "        \n",
    "        # Iterate through each object in the frame metadata until the end\n",
    "        while l_obj is not None:\n",
    "            try:\n",
    "                obj_meta=pyds.NvDsObjectMeta.cast(l_obj.data)\n",
    "                \n",
    "                # Define an analyze_meta function to manipulate metadata\n",
    "                analyze_meta(obj_meta)\n",
    "            except StopIteration:\n",
    "                break\n",
    "                \n",
    "            try: \n",
    "                l_obj=l_obj.next\n",
    "            except StopIteration:\n",
    "                break\n",
    "        \n",
    "        try:\n",
    "            l_frame=l_frame.next\n",
    "        except StopIteration:\n",
    "            break\n",
    "    return Gst.PadProbeReturn.OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d54ba9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "PGIE_CLASS_ID_CAR=0\n",
    "\n",
    "# Define helper function\n",
    "def analyze_meta(obj_meta): \n",
    "    # Only car supports secondary inference\n",
    "    if obj_meta.class_id == PGIE_CLASS_ID_CAR:     \n",
    "        cls_meta=obj_meta.classifier_meta_list\n",
    "        \n",
    "        # Iterate through each class meta until the end\n",
    "        while cls_meta is not None:\n",
    "            cls=pyds.NvDsClassifierMeta.cast(cls_meta.data)\n",
    "            # Get label info\n",
    "            label_info=cls.label_info_list  \n",
    "            \n",
    "            # Iterate through each label info meta until the end\n",
    "            while label_info is not None:\n",
    "                # Cast data type of label from pyds.GList\n",
    "                label_meta=pyds.glist_get_nvds_label_info(label_info.data)\n",
    "                if cls.unique_component_id==2:\n",
    "                    print('\\t Type & Probability = {}% {}'.format(round(label_meta.result_prob*100), label_meta.result_label))\n",
    "                try:\n",
    "                    label_info=label_info.next\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            \n",
    "            try:\n",
    "                cls_meta=cls_meta.next\n",
    "            except StopIteration:\n",
    "                break\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72d5ae3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attached probe\n"
     ]
    }
   ],
   "source": [
    "# Add probe to nvdsosd plugin's sink\n",
    "osdsinkpad=nvosd.get_static_pad(\"sink\")\n",
    "osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe)\n",
    "print('Attached probe')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "579ed514-86c8-43d7-acbc-2a0e881c6bfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Add probe to nvdsosd plugin's sink\n",
    "osdsinkpad=nvosd.get_static_pad(\"sink\")\n",
    "osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe)\n",
    "print('Attached probe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0d637-8c70-473b-a9a6-1d84ab23fe66",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0258b-27bc-4181-91e3-fae9b4f8d71d",
   "metadata": {},
   "source": [
    "<a name='e8'></a>\n",
    "#### Exercise #8 - Start the Pipeline ####\n",
    "\n",
    "**Instructions**: <br>\n",
    "* Execute the cell to add the message handler to the bus. \n",
    "* Modify the `<FIXME>`s only and execute below cell to start the DeepStream pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "acbb7a3a-445e-4fc4-bc5f-f35da5a6c179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added bus message handler\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Create an event loop\n",
    "loop=GLib.MainLoop()\n",
    "\n",
    "# Feed GStreamer bus messages to loop\n",
    "bus=pipeline.get_bus()\n",
    "bus.add_signal_watch()\n",
    "bus.connect (\"message\", bus_call, loop)\n",
    "print('Added bus message handler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c21a132",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: gst-library-error-quark: Configuration file parsing failed (5): gstnvinfer.cpp(794): gst_nvinfer_start (): /GstPipeline:pipeline1/GstNvInfer:secondary-inference:\n",
      "Config file path: spec_files/sgie_config_vehicletypenet_04.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<enum GST_STATE_CHANGE_SUCCESS of type Gst.StateChangeReturn>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start play back and listen to events - this will generate the output_04_raw.mpeg4 file\n",
    "print(\"Starting pipeline \\n\")\n",
    "pipeline.set_state(Gst.State.PLAYING)\n",
    "try:\n",
    "    loop.run()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Cleaning up as the pipeline comes to an end\n",
    "pipeline.set_state(Gst.State.NULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d6343a3-bff4-4370-be24-f7eccd70c54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: gst-library-error-quark: Configuration file parsing failed (5): gstnvinfer.cpp(794): gst_nvinfer_start (): /GstPipeline:pipeline1/GstNvInfer:secondary-inference:\n",
      "Config file path: spec_files/sgie_config_vehicletypenet_04.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<enum GST_STATE_CHANGE_SUCCESS of type Gst.StateChangeReturn>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Starting pipeline \\n\")\n",
    "pipeline.set_state(Gst.State.PLAYING)\n",
    "try:\n",
    "    loop.run()\n",
    "except:\n",
    "    pass\n",
    "pipeline.set_state(Gst.State.NULL)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ead2210-da9e-432c-a38d-bb4827911848",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Start play back and listen to events - this will generate the output_04_raw.mpeg4 file\n",
    "print(\"Starting pipeline \\n\")\n",
    "pipeline.set_state(Gst.State.PLAYING)\n",
    "try:\n",
    "    loop.run()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Cleaning up as the pipeline comes to an end\n",
    "pipeline.set_state(Gst.State.NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d853b-af8c-48fb-bbc8-9512c30a201c",
   "metadata": {},
   "source": [
    "Click ... to show **solution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ff8380",
   "metadata": {},
   "source": [
    "<a name='s3.2'></a>\n",
    "## Viewing the Inference ##\n",
    "In the next step, we convert the video file into a container file before playing it since the MPEG4 encoded video file can't be played directly in JupyterLab. The [FFmpeg](https://ffmpeg.org/) tool is a very fast video and audio converter with the general syntax: \n",
    "* `ffmpeg [global_options] {[input_file_options] -i input_url} ... {[output_file_options] output_url} ...` \n",
    "\n",
    "When using the `ffmpeg` command, the `-i` option lets us read an input URL, the `-loglevel quiet` option suppresses the logs to reduce the output, and the `-y` flag overwrites any existing output file with the same name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00e81868",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video controls  width=\"720\" >\n",
       " <source src=\"data:None;base64,output_04.mp4\" type=\"None\">\n",
       " Your browser does not support the video tag.\n",
       " </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DO NOT CHANGE THIS CELL\n",
    "# Convert MPEG4 video file to MP4 container file\n",
    "!ffmpeg -i /dli/task/output_04_raw.mpeg4 /dli/task/output_04.mp4 \\\n",
    "        -y \\\n",
    "        -loglevel quiet\n",
    "\n",
    "# View the output video\n",
    "# View the output video\n",
    "Video(\"output_04.mp4\", width=720, embed=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec251116",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
